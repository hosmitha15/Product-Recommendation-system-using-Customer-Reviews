{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"1EBLkpp3P2noksDCuj3H1fckcTmGJEAx1","timestamp":1736401518760}],"authorship_tag":"ABX9TyOfJNKv0ExfaL5gIXqHIGsQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"S0yJlnNYRa94"},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","try:\n","    file_path = '/content/nlp.csv'\n","    df = pd.read_csv(file_path)\n","\n","    # Display the first few rows of the dataset\n","    print(\"First few rows of the dataset:\")\n","    print(df.head())\n","\n","    # Check the shape of the dataset\n","    print(\"\\nDataset shape (rows, columns):\", df.shape)\n","\n","    # Check for missing values\n","    print(\"\\nMissing values in each column:\")\n","    print(df.isnull().sum())\n","\n","except FileNotFoundError:\n","    print(f\"Error: File not found at {file_path}\")\n","except pd.errors.ParserError:\n","    print(f\"Error: Could not parse the file at {file_path}. Check the file format.\")\n","except Exception as e:\n","    print(f\"An unexpected error occurred: {e}\")"]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","import nltk\n","\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","\n","print(\"\\nMissing values in each column:\")\n","print(df.isnull().sum())\n","\n","# Filling missing values for certain columns\n","df['title'].fillna('Unknown Title', inplace=True)\n","df['imgUrl'].fillna('No Image URL', inplace=True)\n","df['productURL'].fillna('No Product URL', inplace=True)\n","\n","# Dropping rows with missing values in essential columns\n","df.dropna(subset=['stars', 'reviews', 'price', 'listPrice', 'categoryName'], inplace=True)\n","\n","# Filling missing values for other columns with appropriate defaults\n","df['asin'].fillna('Unknown ASIN', inplace=True)\n","df['isBestSeller'].fillna(False, inplace=True)\n","df['boughtInLastMonth'].fillna(False, inplace=True)\n","\n","print(\"\\nMissing values after preprocessing:\")\n","print(df.isnull().sum())"],"metadata":{"id":"yS9WsN1HRtn6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.reset_index(drop=True, inplace=True)\n","\n","# Remove duplicates\n","df.drop_duplicates(inplace=True)"],"metadata":{"id":"_xowAekRb5r3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Data type conversion\n","df['price'] = pd.to_numeric(df['price'], errors='coerce')  # Convert price to numeric\n","df['listPrice'] = pd.to_numeric(df['listPrice'], errors='coerce')  # Convert listPrice to numeric\n","df['stars'] = pd.to_numeric(df['stars'], errors='coerce')  # Convert stars to numeric\n","\n","# Standardizing text data\n","df['title'] = df['title'].str.lower().str.strip()\n","df['categoryName'] = df['categoryName'].str.lower().str.strip()\n","df['asin'] = df['asin'].str.upper().str.strip()  # Assuming ASINs should be uppercase\n","df['productURL'] = df['productURL'].str.lower().str.strip()\n","df['imgUrl'] = df['imgUrl'].str.lower().str.strip()\n","\n","print(\"\\nData types after conversion:\")\n","print(df.dtypes)\n","\n","print(\"\\nSample data after standardization:\")\n","print(df.head())"],"metadata":{"id":"gtwd_ywmcia7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","# Load the dataset\n","df = pd.read_csv(\"/content/nlp.csv\")\n","\n","# Feature Engineering: Length of the title\n","df['TitleLength'] = df['title'].apply(len)  # Assuming we calculate length for the 'title' column\n","\n","# Outlier detection and treatment using IQR for 'reviews' (assuming 'reviews' represents the number of reviews)\n","Q1 = df['reviews'].quantile(0.25)\n","Q3 = df['reviews'].quantile(0.75)\n","IQR = Q3 - Q1\n","\n","# Filter rows within the IQR range for 'reviews'\n","df = df[(df['reviews'] >= (Q1 - 1.5 * IQR)) & (df['reviews'] <= (Q3 + 1.5 * IQR))]\n","\n","print(\"Feature engineering and outlier treatment complete.\")\n","print(df.head())"],"metadata":{"id":"BFNB5bGqcsIE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.preprocessing import StandardScaler\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","import nltk\n","\n","# Download necessary NLTK resources\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","\n","# Normalization or Standardization of numerical features\n","scaler = StandardScaler()\n","df[['stars', 'reviews', 'price', 'listPrice']] = scaler.fit_transform(df[['stars', 'reviews', 'price', 'listPrice']])\n","\n","# Text Preprocessing for NLP\n","stop_words = set(stopwords.words('english'))\n","lemmatizer = WordNetLemmatizer()\n","\n","# Example text preprocessing for 'title' column\n","df['title'] = df['title'].str.lower().str.split()  # Convert to lowercase and split into words\n","df['title'] = df['title'].apply(lambda x: [word for word in x if word not in stop_words])  # Remove stopwords\n","df['title'] = df['title'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])  # Lemmatize words\n","df['title'] = df['title'].apply(lambda x: ' '.join(x))  # Join words back into a single string\n"],"metadata":{"id":"JS_QqMfGc5e7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def preprocess_text(text):\n","    # Tokenize the text\n","    tokens = text.split()\n","    # Remove stopwords and lemmatize\n","    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n","    # Join tokens back into a single string\n","    return ' '.join(tokens)\n","\n","# Applying the function to the 'title' column (or another text column if needed)\n","df['title'] = df['title'].apply(preprocess_text)"],"metadata":{"id":"QCfkH85PdV2C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Splitting the dataset\n","from sklearn.model_selection import train_test_split\n","\n","# Assuming 'stars' is the target variable and other columns are features\n","X = df.drop('stars', axis=1)  # Features\n","y = df['stars']  # Target variable\n","\n","# Splitting the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Display shapes and first few rows\n","print(\"\\nTraining set shape (rows, columns):\", X_train.shape)\n","print(\"Testing set shape (rows, columns):\", X_test.shape)\n","print(\"\\nFirst few rows of the processed training dataset:\")\n","print(X_train.head())"],"metadata":{"id":"4IF1_W3ndj8e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","from textblob import TextBlob  # TextBlob for performing NLP tasks and sentiment analysis\n","import numpy as np\n","\n","# Sentiment Analysis\n","def get_sentiment(text):\n","    return TextBlob(text).sentiment.polarity  # Returns a sentiment score between -1 and 1\n","\n","# Apply sentiment analysis to the 'title' column (assuming it's the primary text column)\n","df['SentimentScore'] = df['title'].apply(get_sentiment)\n","\n","# Helpfulness Ratio\n","# Replace HelpfulnessNumerator and HelpfulnessDenominator with 'reviews' (numerator) and 'stars' (denominator)\n","df['HelpfulnessRatio'] = df['reviews'] / df['stars'].replace(0, np.nan)  # Avoid division by zero"],"metadata":{"id":"cHCAj3cjd0Rx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['SentimentScore'] = df['title'].apply(get_sentiment)  # Apply sentiment analysis to the 'title' column\n","\n","# Helpfulness Ratio\n","df['HelpfulnessRatio'] = df['reviews'] / df['stars'].replace(0, np.nan)  # Calculate helpfulness ratio (assuming reviews and stars)\n","\n","# Check if the required columns exist in the dataset\n","if all(col in df.columns for col in ['asin', 'title', 'reviews', 'stars', 'categoryName', 'isBestSeller']):\n","\n","    # User Profiles aggregation (grouping by 'asin' as the unique product identifier)\n","    user_profiles = df.groupby('asin').agg(\n","        AverageRating=('stars', 'mean'),\n","        PreferredCategories=('categoryName', lambda x: x.mode()[0] if not x.mode().empty else np.nan),  # Most common categoryName\n","        TotalReviews=('reviews', 'sum')\n","    ).reset_index()\n","\n","    # Product Profiles aggregation\n","    product_profiles = df.groupby('asin').agg(\n","        AverageScore=('stars', 'mean'),\n","        TotalReviews=('reviews', 'sum'),\n","        AverageSentimentScore=('SentimentScore', 'mean'),  # Average sentiment score\n","        AverageHelpfulnessRatio=('HelpfulnessRatio', 'mean')  # Average helpfulness ratio\n","    ).reset_index()\n","\n","    # Display the user profiles\n","    print(\"User Profiles:\")\n","    print(user_profiles.head())\n","\n","    # Display the product profiles\n","    print(\"\\nProduct Profiles:\")\n","    print(product_profiles.head())\n","else:\n","    print(\"The required columns are not present in the dataset.\")"],"metadata":{"id":"xpom4R_EeDta"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install scikit-surprise"],"metadata":{"id":"WDzGIUiDeTf3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from surprise import Dataset, Reader, SVD\n","from surprise.model_selection import train_test_split\n","from surprise import accuracy\n","\n","# Assuming 'df' contains the necessary columns:\n","# 'asin' as ProductId, 'stars' as the rating score, and 'UserId' for user interaction.\n","\n","# Collaborative Filtering\n","reader = Reader(rating_scale=(1, 5))  # Rating scale\n","data = Dataset.load_from_df(df[['title', 'asin', 'stars']], reader)\n","\n","# Split the dataset into training and testing sets\n","trainset, testset = train_test_split(data, test_size=0.2)\n","\n","# Initialize SVD model\n","model = SVD()\n","\n","# Train the model\n","model.fit(trainset)\n","\n","# Make predictions\n","predictions = model.test(testset)\n","\n","# Calculate RMSE\n","rmse = accuracy.rmse(predictions)\n","print(f'RMSE of SVD: {rmse}')"],"metadata":{"id":"GDOfbHWceZ99"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Content-Based Filtering using TF-IDF and Cosine Similarity\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity"],"metadata":{"id":"vKkisiKsgBeS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# Load the dataset\n","df = pd.read_csv(\"/content/nlp.csv\")\n","\n","# Assuming 'title' or 'reviews' contains the text data for content-based filtering\n","# You can choose one of these columns for product descriptions or reviews\n","\n","# Using 'title' for product description (you can also use 'reviews' if that's more suitable)\n","tfidf = TfidfVectorizer(stop_words='english')\n","tfidf_matrix = tfidf.fit_transform(df['title'])  # or use df['reviews'] if needed\n","\n","# Now, tfidf_matrix contains the TF-IDF representation of the product titles or reviews.\n"],"metadata":{"id":"P21lV7yrgL5J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Compute cosine similarity matrix\n","from sklearn.metrics.pairwise import cosine_similarity\n","cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n","df=pd.read_csv(\"/content/nlp.csv\");\n","# Function to get recommendations based on cosine similarity"],"metadata":{"id":"8JbaDWRxgQWJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics.pairwise import cosine_similarity\n","\n","def get_content_based_recommendations(product_name):\n","    # Check if the product_name exists in the DataFrame\n","    if product_name not in df['title'].values:\n","        raise ValueError(f\"Product name '{product_name}' not found in the DataFrame.\")\n","\n","    # Get the index of the product\n","    try:\n","        idx = df.index[df['title'] == product_name][0]\n","    except IndexError:\n","        raise ValueError(f\"Product name '{product_name}' not found in the DataFrame index.\")\n","\n","    # Get the pairwise similarity scores of all products with that product\n","    sim_scores = list(enumerate(cosine_sim[idx]))\n","\n","    # Sort the products based on the similarity scores\n","    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n","\n","    # Get the scores of the 10 most similar products\n","    sim_scores = sim_scores[1:min(11, len(sim_scores))]\n","\n","    # Get the product indices, ensuring they are within the DataFrame bounds\n","    product_indices = [i[0] for i in sim_scores if 0 <= i[0] < len(df)]\n","\n","    # Return the recommendations, or an empty DataFrame if no similar products are found\n","    if not product_indices:\n","        return pd.DataFrame()  # Return an empty DataFrame if no valid indices\n","\n","    return df.iloc[product_indices]\n","\n","# Example usage\n","product_name = 'Disney Princess E0274 Royal Shimmer Belle Doll'  # Replace with a valid product title from your DataFrame\n","try:\n","    recommended_products = get_content_based_recommendations(product_name=product_name)\n","    print(\"Content-Based Recommendations:\")\n","    print(recommended_products[['asin', 'title']])  # Assuming 'title' is the product name column\n","except ValueError as e:\n","    print(e)\n","except IndexError as e:\n","    print(\"Index error encountered:\", e)"],"metadata":{"id":"nZBN6XKtglWe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics.pairwise import cosine_similarity\n","\n","def get_content_based_recommendations(product_name):\n","    # Check if the product_name exists in the DataFrame\n","    if product_name not in df['title'].values:\n","        raise ValueError(f\"Product name '{product_name}' not found in the DataFrame.\")\n","\n","    # Get the index of the product\n","    try:\n","        idx = df.index[df['title'] == product_name][0]\n","    except IndexError:\n","        raise ValueError(f\"Product name '{product_name}' not found in the DataFrame index.\")\n","\n","    # Get the pairwise similarity scores of all products with that product\n","    sim_scores = list(enumerate(cosine_sim[idx]))\n","\n","    # Sort the products based on the similarity scores\n","    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n","\n","    # Get the scores of the top 10 most similar products (excluding the input product itself)\n","    sim_scores = sim_scores[1:min(11, len(sim_scores))]\n","\n","    # Get the product indices\n","    product_indices = [i[0] for i in sim_scores if 0 <= i[0] < len(df)]\n","\n","    # Filter based on reviews, stars, and price to recommend the top product\n","    recommendations = df.iloc[product_indices].copy()  # Ensure we're working with a copy\n","\n","    # Calculate the weighted score for each product\n","    recommendations['weighted_score'] = (\n","        recommendations['stars'] * 0.5 +  # Star rating weight\n","        recommendations['reviews'] * 0.3 +  # Review count weight\n","        (1 / recommendations['price']) * 0.2  # Price weight (assuming lower price is better)\n","    )\n","\n","    # Sort by the weighted score and return the top 1 product\n","    best_recommendation = recommendations.sort_values(by='weighted_score', ascending=False).iloc[0]\n","\n","    return best_recommendation\n","\n","# Example usage\n","product_name = 'Disney Princess E0274 Royal Shimmer Belle Doll'  # Replace with a valid product title\n","try:\n","    recommended_product = get_content_based_recommendations(product_name=product_name)\n","    print(\"Top Content-Based Recommendation:\")\n","    print(recommended_product[['asin', 'title']])  # Display the top recommended product\n","except ValueError as e:\n","    print(e)\n","except IndexError as e:\n","    print(\"Index error encountered:\", e)"],"metadata":{"id":"eAgGKV-ChIJJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n"],"metadata":{"id":"yJ_bCBY4jJ2d"},"execution_count":null,"outputs":[]}]}